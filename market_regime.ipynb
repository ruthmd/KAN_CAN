{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff37cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optax yfinance lxml plotly hmmlearn \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26041c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, jit\n",
    "import numpy as np\n",
    "import optax\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Callable, Any\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, timedelta\n",
    "import yfinance as yf\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# KAN Layer implementation for market regime detection\n",
    "class KANLayer:\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_basis: int = 30, \n",
    "                 domain=(-3.0, 3.0), key=None):\n",
    "        \"\"\"Initialize a KAN layer with learnable activation functions.\"\"\"\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        \n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "        \n",
    "        # Initialize weights for linear transformation\n",
    "        self.weights = jax.random.normal(key1, (input_dim, output_dim)) * 0.1\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.biases = jax.random.normal(key2, (output_dim,)) * 0.01\n",
    "        \n",
    "        # Grid points for activation function representation\n",
    "        self.grid_points = jnp.linspace(domain[0], domain[1], num_basis)\n",
    "        \n",
    "        # Initialize activation function values with different shapes\n",
    "        activations_list = []\n",
    "        for i in range(output_dim):\n",
    "            subkey = jax.random.fold_in(key3, i)\n",
    "            init_type = jax.random.randint(subkey, (), 0, 4)\n",
    "            \n",
    "            if init_type == 0:  # Linear-like\n",
    "                act = self.grid_points\n",
    "            elif init_type == 1:  # ReLU-like\n",
    "                act = jnp.maximum(0, self.grid_points)\n",
    "            elif init_type == 2:  # Sigmoid-like\n",
    "                act = 1.0 / (1.0 + jnp.exp(-self.grid_points))\n",
    "            else:  # Tanh-like\n",
    "                act = jnp.tanh(self.grid_points)\n",
    "            \n",
    "            # Add noise to break symmetry\n",
    "            act = act + jax.random.normal(subkey, (num_basis,)) * 0.05\n",
    "            activations_list.append(act)\n",
    "        \n",
    "        # Stack into a matrix: (output_dim, num_basis)\n",
    "        self.activations = jnp.stack(activations_list)\n",
    "        \n",
    "        # Store domain for clipping\n",
    "        self.domain = domain\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass through the KAN layer.\"\"\"\n",
    "        # Linear transformation\n",
    "        z = jnp.dot(x, self.weights) + self.biases  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        # Apply learned activation functions by interpolation\n",
    "        z_clipped = jnp.clip(z, self.domain[0], self.domain[1])\n",
    "        \n",
    "        def apply_activation(z_i, i):\n",
    "            \"\"\"Apply the i-th activation function to z_i using linear interpolation.\"\"\"\n",
    "            idx = jnp.searchsorted(self.grid_points, z_i) - 1\n",
    "            idx = jnp.clip(idx, 0, len(self.grid_points) - 2)\n",
    "            \n",
    "            x0 = self.grid_points[idx]\n",
    "            x1 = self.grid_points[idx + 1]\n",
    "            y0 = self.activations[i, idx]\n",
    "            y1 = self.activations[i, idx + 1]\n",
    "            \n",
    "            t = (z_i - x0) / (x1 - x0)\n",
    "            return y0 + t * (y1 - y0)\n",
    "        \n",
    "        # Apply activation function for each element in the batch and each output dimension\n",
    "        output = jnp.zeros_like(z)\n",
    "        for i in range(z.shape[1]):  # For each output dimension\n",
    "            output = output.at[:, i].set(vmap(lambda z_i: apply_activation(z_i, i))(z_clipped[:, i]))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Regime Detection KAN model\n",
    "class RegimeDetectionKAN:\n",
    "    def __init__(self, input_dim: int, latent_dim: int = 5, hidden_dims: List[int] = [64, 32], \n",
    "                 num_basis: int = 30, domain=(-3.0, 3.0), key=None):\n",
    "        \"\"\"Initialize a KAN model for market regime detection.\"\"\"\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        \n",
    "        keys = jax.random.split(key, len(hidden_dims) + 1)\n",
    "        \n",
    "        # Initialize encoder layers\n",
    "        self.encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layer = KANLayer(prev_dim, hidden_dim, num_basis, domain, keys[i])\n",
    "            self.encoder_layers.append(layer)\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Latent space representation layer (embedding space for regime detection)\n",
    "        self.latent_layer = KANLayer(prev_dim, latent_dim, num_basis, domain, keys[-1])\n",
    "        \n",
    "        # Store latent dimension for later use\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def encode(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Encode inputs to latent space representation.\"\"\"\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Final encoding to latent space\n",
    "        latent = self.latent_layer(x)\n",
    "        \n",
    "        return latent\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        latent = self.encode(x)\n",
    "        return latent\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"Get model parameters as a flat dictionary.\"\"\"\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            params[f'encoder_layer_{i}_weights'] = layer.weights\n",
    "            params[f'encoder_layer_{i}_biases'] = layer.biases\n",
    "            params[f'encoder_layer_{i}_activations'] = layer.activations\n",
    "        \n",
    "        params['latent_layer_weights'] = self.latent_layer.weights\n",
    "        params['latent_layer_biases'] = self.latent_layer.biases\n",
    "        params['latent_layer_activations'] = self.latent_layer.activations\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def update_params(self, params):\n",
    "        \"\"\"Update model parameters from a flat dictionary.\"\"\"\n",
    "        for i, layer in enumerate(self.encoder_layers):\n",
    "            layer.weights = params[f'encoder_layer_{i}_weights']\n",
    "            layer.biases = params[f'encoder_layer_{i}_biases']\n",
    "            layer.activations = params[f'encoder_layer_{i}_activations']\n",
    "        \n",
    "        self.latent_layer.weights = params['latent_layer_weights']\n",
    "        self.latent_layer.biases = params['latent_layer_biases']\n",
    "        self.latent_layer.activations = params['latent_layer_activations']\n",
    "\n",
    "# Contrastive loss for unsupervised regime detection\n",
    "def contrastive_loss(latent_representations, temperature=0.1):\n",
    "    \"\"\"Contrastive loss for learning meaningful latent representations.\"\"\"\n",
    "    # Normalize latent representations\n",
    "    latent_norm = latent_representations / jnp.sqrt(jnp.sum(latent_representations**2, axis=1, keepdims=True) + 1e-8)\n",
    "    \n",
    "    # Compute similarity matrix\n",
    "    similarity = jnp.matmul(latent_norm, latent_norm.T) / temperature\n",
    "    \n",
    "    # Create labels (neighboring points are considered positive pairs)\n",
    "    batch_size = latent_representations.shape[0]\n",
    "    \n",
    "    # Define positive pairs as the neighboring points (e.g., t and t+1)\n",
    "    # Assumption: data is in temporal order\n",
    "    positive_mask = jnp.zeros((batch_size, batch_size))\n",
    "    \n",
    "    # Diagonal elements are excluded (self-similarity)\n",
    "    pos_indices = jnp.arange(batch_size - 1)\n",
    "    positive_mask = positive_mask.at[pos_indices, pos_indices + 1].set(1.0)\n",
    "    positive_mask = positive_mask + positive_mask.T  # Symmetrize\n",
    "    \n",
    "    # All other pairs are considered negative\n",
    "    negative_mask = 1.0 - positive_mask - jnp.eye(batch_size)\n",
    "    \n",
    "    # Compute loss for positive pairs\n",
    "    pos_similarity = jnp.sum(similarity * positive_mask, axis=1) / (jnp.sum(positive_mask, axis=1) + 1e-8)\n",
    "    \n",
    "    # Compute loss for negative pairs (using log-sum-exp trick for numerical stability)\n",
    "    neg_similarity = jnp.log(jnp.sum(jnp.exp(similarity) * negative_mask, axis=1) + 1e-8)\n",
    "    \n",
    "    # Contrastive loss\n",
    "    loss = -jnp.mean(pos_similarity - neg_similarity)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Temporal smoothness loss to encourage smooth regime transitions\n",
    "def temporal_smoothness_loss(latent_representations, lambda_smooth=1.0):\n",
    "    \"\"\"Temporal smoothness loss to encourage smooth regime transitions.\"\"\"\n",
    "    # Calculate differences between consecutive latent representations\n",
    "    diffs = latent_representations[1:] - latent_representations[:-1]\n",
    "    \n",
    "    # Squared L2 norm of differences\n",
    "    squared_diffs = jnp.sum(diffs**2, axis=1)\n",
    "    \n",
    "    # Mean squared difference\n",
    "    smoothness_loss = jnp.mean(squared_diffs)\n",
    "    \n",
    "    return lambda_smooth * smoothness_loss\n",
    "\n",
    "# Helper function to extract values from a parameter dictionary\n",
    "def extract_params(params, layer_idx, param_type):\n",
    "    \"\"\"Extract parameters with static indexing to avoid JIT string issues.\"\"\"\n",
    "    key = f'encoder_layer_{layer_idx}_{param_type}'\n",
    "    return params[key]\n",
    "\n",
    "# JIT-compatible model application function\n",
    "def apply_model_jit(params, x, num_encoder_layers):\n",
    "    \"\"\"JIT-compatible version of apply_model that uses fixed layer structure.\"\"\"\n",
    "    grid_points = jnp.linspace(-3.0, 3.0, 30)  # Assuming fixed num_basis=30\n",
    "    \n",
    "    # Apply each encoder layer\n",
    "    for i in range(num_encoder_layers):\n",
    "        # Static parameter access patterns - this key construction happens outside JIT\n",
    "        weights_key = f'encoder_layer_{i}_weights'\n",
    "        biases_key = f'encoder_layer_{i}_biases' \n",
    "        activations_key = f'encoder_layer_{i}_activations'\n",
    "        \n",
    "        weights = params[weights_key]\n",
    "        biases = params[biases_key]\n",
    "        activations = params[activations_key]\n",
    "        \n",
    "        # Linear transformation\n",
    "        x = jnp.dot(x, weights) + biases\n",
    "        \n",
    "        # Apply activations through vectorized interpolation\n",
    "        x_clipped = jnp.clip(x, -3.0, 3.0)\n",
    "        \n",
    "        # Process each output dimension\n",
    "        output_cols = []\n",
    "        for j in range(x.shape[1]):\n",
    "            # Find indices for interpolation\n",
    "            idx = jnp.searchsorted(grid_points, x_clipped[:, j]) - 1\n",
    "            idx = jnp.clip(idx, 0, len(grid_points) - 2)\n",
    "            \n",
    "            # Get grid points and activation values\n",
    "            x0 = grid_points[idx]\n",
    "            x1 = grid_points[idx + 1]\n",
    "            y0 = activations[j, idx]\n",
    "            y1 = activations[j, idx + 1]\n",
    "            \n",
    "            # Linear interpolation\n",
    "            t = (x_clipped[:, j] - x0) / (x1 - x0)\n",
    "            output_cols.append(y0 + t * (y1 - y0))\n",
    "        \n",
    "        x = jnp.column_stack(output_cols)\n",
    "    \n",
    "    # Apply latent layer\n",
    "    weights = params['latent_layer_weights']\n",
    "    biases = params['latent_layer_biases']\n",
    "    activations = params['latent_layer_activations']\n",
    "    \n",
    "    # Linear transformation\n",
    "    x = jnp.dot(x, weights) + biases\n",
    "    \n",
    "    # Apply activations\n",
    "    x_clipped = jnp.clip(x, -3.0, 3.0)\n",
    "    \n",
    "    # Process each output dimension\n",
    "    output_cols = []\n",
    "    for j in range(x.shape[1]):\n",
    "        # Find indices for interpolation\n",
    "        idx = jnp.searchsorted(grid_points, x_clipped[:, j]) - 1\n",
    "        idx = jnp.clip(idx, 0, len(grid_points) - 2)\n",
    "        \n",
    "        # Get grid points and activation values\n",
    "        x0 = grid_points[idx]\n",
    "        x1 = grid_points[idx + 1]\n",
    "        y0 = activations[j, idx]\n",
    "        y1 = activations[j, idx + 1]\n",
    "        \n",
    "        # Linear interpolation\n",
    "        t = (x_clipped[:, j] - x0) / (x1 - x0)\n",
    "        output_cols.append(y0 + t * (y1 - y0))\n",
    "    \n",
    "    return jnp.column_stack(output_cols)\n",
    "\n",
    "# Combined loss function for JIT compatibility\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def regime_loss_fn_jit(params, X, num_encoder_layers, lambda_smooth=1.0, lambda_reg=0.001):\n",
    "    \"\"\"JIT-compatible loss function with static argument for num_encoder_layers.\"\"\"\n",
    "    # Apply the model to get latent representations\n",
    "    latent = apply_model_jit(params, X, num_encoder_layers)\n",
    "    \n",
    "    # Contrastive loss\n",
    "    contrast_loss = contrastive_loss(latent)\n",
    "    \n",
    "    # Temporal smoothness loss\n",
    "    smoothness_loss = temporal_smoothness_loss(latent, lambda_smooth)\n",
    "    \n",
    "    # Activation smoothness regularization\n",
    "    activation_reg = 0.0\n",
    "    for i in range(num_encoder_layers):\n",
    "        activations_key = f'encoder_layer_{i}_activations'\n",
    "        activations = params[activations_key]\n",
    "        # Calculate second derivatives (approximation)\n",
    "        second_deriv = activations[:, 2:] - 2 * activations[:, 1:-1] + activations[:, :-2]\n",
    "        activation_reg += jnp.mean(second_deriv ** 2)\n",
    "    \n",
    "    # Add latent layer activations\n",
    "    activations = params['latent_layer_activations']\n",
    "    second_deriv = activations[:, 2:] - 2 * activations[:, 1:-1] + activations[:, :-2]\n",
    "    activation_reg += jnp.mean(second_deriv ** 2)\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = contrast_loss + smoothness_loss + lambda_reg * activation_reg\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Training step with JIT compilation\n",
    "@partial(jit, static_argnums=(2,))\n",
    "def train_step_jit(params, X, num_encoder_layers, opt_state, lambda_smooth=1.0, lambda_reg=0.001):\n",
    "    \"\"\"JIT-compatible training step with static argument for structure.\"\"\"\n",
    "    # Create a partial function with fixed num_encoder_layers\n",
    "    loss_fn = lambda p: regime_loss_fn_jit(p, X, num_encoder_layers, lambda_smooth, lambda_reg)\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_value\n",
    "\n",
    "# Modified training function\n",
    "def train_regime_model(model, X_train, num_epochs=100, batch_size=64, lambda_smooth=1.0, lambda_reg=0.001):\n",
    "    \"\"\"Train the regime detection model.\"\"\"\n",
    "    params = model.params\n",
    "    num_encoder_layers = len(model.encoder_layers)\n",
    "    \n",
    "    num_samples = X_train.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    global optimizer\n",
    "    optimizer = optax.adam(learning_rate=0.001)\n",
    "    \n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # For regime detection, we maintain temporal order within batches\n",
    "        # But shuffle the starting points of each batch\n",
    "        starts = np.random.permutation(num_batches) * batch_size\n",
    "        starts = np.clip(starts, 0, num_samples - batch_size)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for start_idx in starts:\n",
    "            end_idx = start_idx + batch_size\n",
    "            X_batch = X_train[start_idx:end_idx]\n",
    "            \n",
    "            # Use JIT-compatible training step with num_encoder_layers as static argument\n",
    "            params, opt_state, batch_loss = train_step_jit(\n",
    "                params, X_batch, num_encoder_layers, opt_state, lambda_smooth, lambda_reg\n",
    "            )\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "        epoch_loss /= len(starts)\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss:.6f}\")\n",
    "    \n",
    "    # Update model with trained parameters\n",
    "    model.update_params(params)\n",
    "    return params, losses\n",
    "\n",
    "# Detect regimes using the trained model\n",
    "def detect_regimes(model, params, X, num_regimes=4):\n",
    "    \"\"\"Detect market regimes using the trained model.\"\"\"\n",
    "    model.update_params(params)\n",
    "    latent = model(X)\n",
    "    \n",
    "    # Convert to numpy for clustering\n",
    "    latent_np = np.array(latent)\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=num_regimes, random_state=42, n_init=10)\n",
    "    detected_regimes = kmeans.fit_predict(latent_np)\n",
    "    \n",
    "    # Apply temporal smoothing to regime labels\n",
    "    # This helps avoid rapid switching between regimes\n",
    "    window_size = 5\n",
    "    smoothed_regimes = np.copy(detected_regimes)\n",
    "    \n",
    "    for i in range(len(detected_regimes)):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(len(detected_regimes), i + window_size // 2 + 1)\n",
    "        window = detected_regimes[start:end]\n",
    "        # Get most common regime in window\n",
    "        smoothed_regimes[i] = np.bincount(window).argmax()\n",
    "    \n",
    "    return smoothed_regimes, latent_np\n",
    "\n",
    "# Download and prepare financial data\n",
    "def download_market_data(tickers, start_date, end_date, feature_window=20):\n",
    "    \"\"\"\n",
    "    Download market data for a list of tickers and prepare features.\n",
    "    \n",
    "    Args:\n",
    "        tickers: List of ticker symbols\n",
    "        start_date: Start date for data download\n",
    "        end_date: End date for data download\n",
    "        feature_window: Window size for creating features\n",
    "        \n",
    "    Returns:\n",
    "        X: Features array\n",
    "        dates: Dates corresponding to features\n",
    "        returns_dict: Dictionary of returns for each ticker\n",
    "    \"\"\"\n",
    "    # Download data\n",
    "    print(f\"Downloading data for {len(tickers)} tickers...\")\n",
    "    data = {}\n",
    "    returns_dict = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            # Download data\n",
    "            df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(f\"No data found for {ticker}, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate returns\n",
    "            df['Return'] = df['Close'].pct_change()\n",
    "            df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "            \n",
    "            # Calculate volatility\n",
    "            df['Volatility'] = df['Log_Return'].rolling(window=20).std()\n",
    "            \n",
    "            # Create additional features\n",
    "            df['RSI'] = calculate_rsi(df['Close'])\n",
    "            df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "            df['MA_50'] = df['Close'].rolling(window=50).mean()\n",
    "            df['MA_Ratio'] = df['MA_10'] / df['MA_50']\n",
    "            \n",
    "            # Store data\n",
    "            data[ticker] = df\n",
    "            returns_dict[ticker] = df['Return']\n",
    "            \n",
    "            print(f\"Downloaded {len(df)} days of data for {ticker}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data for {ticker}: {e}\")\n",
    "    \n",
    "    # Check if we have any data\n",
    "    if not data:\n",
    "        raise ValueError(\"No data could be downloaded for any of the provided tickers. Please check the ticker symbols or date range.\")\n",
    "    \n",
    "    # Align dates across all tickers\n",
    "    common_dates = None\n",
    "    \n",
    "    for ticker in data:\n",
    "        if common_dates is None:\n",
    "            common_dates = set(data[ticker].index)\n",
    "        else:\n",
    "            common_dates = common_dates.intersection(set(data[ticker].index))\n",
    "    \n",
    "    # Check if we have any common dates\n",
    "    if not common_dates:\n",
    "        raise ValueError(\"No common dates found across all tickers. Try using a different date range or set of tickers.\")\n",
    "        \n",
    "    common_dates = sorted(list(common_dates))\n",
    "    print(f\"Found {len(common_dates)} common dates across all tickers\")\n",
    "    \n",
    "    # Create features for each ticker\n",
    "    feature_dfs = []\n",
    "    \n",
    "    for ticker in data:\n",
    "        df = data[ticker].loc[common_dates].copy()\n",
    "        # Keep only the features we want\n",
    "        features = df[['Log_Return', 'Volatility', 'RSI', 'MA_Ratio']].copy()\n",
    "        # Add ticker prefix to column names\n",
    "        features.columns = [f\"{ticker}_{col}\" for col in features.columns]\n",
    "        feature_dfs.append(features)\n",
    "    \n",
    "    # Combine features across tickers\n",
    "    combined_features = pd.concat(feature_dfs, axis=1)\n",
    "    combined_features = combined_features.dropna()\n",
    "    \n",
    "    # Check if we have enough data for the feature window\n",
    "    if len(combined_features) <= feature_window:\n",
    "        raise ValueError(f\"Not enough data ({len(combined_features)} days) for the specified feature window ({feature_window} days). Try using a longer date range.\")\n",
    "    \n",
    "    # Create rolling window features\n",
    "    X = []\n",
    "    dates = []\n",
    "    \n",
    "    for i in range(feature_window, len(combined_features)):\n",
    "        # Use a window of market data as features\n",
    "        x_t = combined_features.iloc[i-feature_window:i].values.flatten()\n",
    "        X.append(x_t)\n",
    "        dates.append(combined_features.index[i])\n",
    "    \n",
    "    # Check if we have any data points\n",
    "    if not X:\n",
    "        raise ValueError(\"No data points could be created. Check for NaN values in your data or try a different date range.\")\n",
    "        \n",
    "    X = np.array(X)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X, dates, returns_dict\n",
    "\n",
    "# Calculate RSI\n",
    "def calculate_rsi(prices, window=14):\n",
    "    \"\"\"Calculate Relative Strength Index (RSI).\"\"\"\n",
    "    # Calculate price changes\n",
    "    delta = prices.diff()\n",
    "    \n",
    "    # Create gain and loss series\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    \n",
    "    # Calculate average gain and loss\n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Calculate RS\n",
    "    rs = avg_gain / avg_loss\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "# Visualize regimes\n",
    "def visualize_regimes(latent_representations, detected_regimes, dates):\n",
    "    \"\"\"Visualize detected regimes in the latent space.\"\"\"\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(latent_representations)-1))\n",
    "    latent_2d = tsne.fit_transform(latent_representations)\n",
    "    \n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot detected regimes\n",
    "    scatter = ax.scatter(latent_2d[:, 0], latent_2d[:, 1], c=detected_regimes, cmap='viridis', alpha=0.7, s=50)\n",
    "    ax.set_title('Detected Market Regimes (KAN)', fontsize=15)\n",
    "    ax.set_xlabel('t-SNE dimension 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE dimension 2', fontsize=12)\n",
    "    plt.colorbar(scatter, ax=ax, label='Regime')\n",
    "    \n",
    "    # Add a legend\n",
    "    unique_regimes = np.unique(detected_regimes)\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.viridis(i/len(unique_regimes)), \n",
    "                                 markersize=10, label=f'Regime {i}') for i in unique_regimes]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot regime transitions\n",
    "def plot_regime_transitions(detected_regimes, dates, returns_dict, ticker_to_plot=None):\n",
    "    \"\"\"Plot regime transitions over time with market returns in the background.\"\"\"\n",
    "    # Ensure dates is a list\n",
    "    if not isinstance(dates, list):\n",
    "        dates = list(dates)\n",
    "    \n",
    "    # Create a pandas Series for the regimes\n",
    "    regime_series = pd.Series(detected_regimes, index=dates)\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True, gridspec_kw={'height_ratios': [1, 3]})\n",
    "    \n",
    "    # Plot detected regimes\n",
    "    ax1.plot(regime_series.index, regime_series.values, 'k-', linewidth=2)\n",
    "    ax1.set_title('Detected Market Regimes (KAN)', fontsize=15)\n",
    "    ax1.set_ylabel('Regime', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set y-axis to show integer values only\n",
    "    ax1.set_yticks(np.unique(detected_regimes))\n",
    "    \n",
    "    # Plot returns for the specified ticker or the first ticker in the dict\n",
    "    if ticker_to_plot is None or ticker_to_plot not in returns_dict:\n",
    "        ticker_to_plot = list(returns_dict.keys())[0]\n",
    "    \n",
    "    # Get returns for the ticker\n",
    "    returns = returns_dict[ticker_to_plot]\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    \n",
    "    # Plot cumulative returns\n",
    "    ax2.plot(cum_returns.index, cum_returns.values, 'b-', linewidth=1.5)\n",
    "    ax2.set_title(f'Cumulative Returns for {ticker_to_plot}', fontsize=15)\n",
    "    ax2.set_xlabel('Date', fontsize=12)\n",
    "    ax2.set_ylabel('Cumulative Return', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark regime changes with vertical lines\n",
    "    regime_changes = np.where(np.diff(detected_regimes) != 0)[0]\n",
    "    for change_idx in regime_changes:\n",
    "        if change_idx < len(dates) - 1:  # Ensure we don't go out of bounds\n",
    "            change_date = dates[change_idx + 1]\n",
    "            ax2.axvline(change_date, color='r', linestyle='--', alpha=0.5)\n",
    "            ax1.axvline(change_date, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add colored backgrounds for different regimes\n",
    "    unique_regimes = np.unique(detected_regimes)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_regimes)))\n",
    "    \n",
    "    prev_date = dates[0]\n",
    "    prev_regime = detected_regimes[0]\n",
    "    \n",
    "    for i in range(1, len(dates)):\n",
    "        if detected_regimes[i] != prev_regime or i == len(dates) - 1:\n",
    "            # Add colored background for this regime period\n",
    "            ax2.axvspan(prev_date, dates[i-1], alpha=0.2, color=colors[prev_regime])\n",
    "            # Update for next period\n",
    "            prev_date = dates[i]\n",
    "            prev_regime = detected_regimes[i]\n",
    "    \n",
    "    # Add the last period\n",
    "    ax2.axvspan(prev_date, dates[-1], alpha=0.2, color=colors[prev_regime])\n",
    "    \n",
    "    # Add a legend for the regimes\n",
    "    legend_elements = [plt.Line2D([0], [0], color=colors[i], lw=8, alpha=0.5, \n",
    "                                  label=f'Regime {i}') for i in unique_regimes]\n",
    "    ax2.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Analyze regime characteristics\n",
    "def analyze_regime_characteristics(detected_regimes, dates, returns_dict):\n",
    "    \"\"\"Analyze characteristics of each detected regime.\"\"\"\n",
    "    # Create a pandas Series for the regimes\n",
    "    regime_series = pd.Series(detected_regimes, index=dates)\n",
    "    \n",
    "    # Initialize a dictionary to store regime characteristics\n",
    "    regime_stats = {}\n",
    "    \n",
    "    # Get unique regimes\n",
    "    unique_regimes = np.unique(detected_regimes)\n",
    "    \n",
    "    # Create a DataFrame for easier analysis\n",
    "    data = pd.DataFrame(index=dates)\n",
    "    \n",
    "    # Add ticker returns to the DataFrame\n",
    "    for ticker in returns_dict:\n",
    "        # Filter returns for the dates we have\n",
    "        ticker_returns = returns_dict[ticker].reindex(dates)\n",
    "        data[ticker] = ticker_returns\n",
    "    \n",
    "    # Calculate statistics for each regime\n",
    "    for regime in unique_regimes:\n",
    "        regime_dates = regime_series[regime_series == regime].index\n",
    "        regime_data = data.loc[regime_dates]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_returns = regime_data.mean()\n",
    "        volatility = regime_data.std()\n",
    "        sharpe = mean_returns / volatility\n",
    "        correlation = regime_data.corr()\n",
    "        \n",
    "        # Store statistics\n",
    "        regime_stats[regime] = {\n",
    "            'dates': regime_dates,\n",
    "            'num_periods': len(regime_dates),\n",
    "            'mean_returns': mean_returns,\n",
    "            'volatility': volatility,\n",
    "            'sharpe': sharpe,\n",
    "            'correlation': correlation,\n",
    "            'start_date': regime_dates[0] if len(regime_dates) > 0 else None,\n",
    "            'end_date': regime_dates[-1] if len(regime_dates) > 0 else None\n",
    "        }\n",
    "    \n",
    "    return regime_stats\n",
    "\n",
    "# Visualize regime characteristics\n",
    "def visualize_regime_characteristics(regime_stats, tickers):\n",
    "    \"\"\"Visualize statistical characteristics of each regime.\"\"\"\n",
    "    # Number of regimes\n",
    "    num_regimes = len(regime_stats)\n",
    "    \n",
    "    if num_regimes == 0:\n",
    "        print(\"No regime statistics available to visualize\")\n",
    "        return None\n",
    "    \n",
    "    # Create figure with multiple subplots - one row per regime\n",
    "    fig, axs = plt.subplots(num_regimes, 3, figsize=(16, 5 * num_regimes))\n",
    "    \n",
    "    if num_regimes == 1:\n",
    "        axs = axs.reshape(1, -1)\n",
    "    \n",
    "    for i, regime in enumerate(sorted(regime_stats.keys())):\n",
    "        stats = regime_stats[regime]\n",
    "        \n",
    "        # Plot mean returns\n",
    "        axs[i, 0].bar(tickers, [stats['mean_returns'][ticker] for ticker in tickers])\n",
    "        axs[i, 0].set_title(f'Regime {regime}: Mean Daily Returns')\n",
    "        axs[i, 0].set_xlabel('Ticker')\n",
    "        axs[i, 0].set_ylabel('Mean Return')\n",
    "        axs[i, 0].grid(True, alpha=0.3)\n",
    "        axs[i, 0].set_xticklabels(tickers, rotation=45)\n",
    "        \n",
    "        # Add a horizontal line at y=0\n",
    "        axs[i, 0].axhline(y=0, color='r', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Add period information as text\n",
    "        date_info = f\"Period: {stats['start_date'].strftime('%Y-%m-%d')} to {stats['end_date'].strftime('%Y-%m-%d')}\"\n",
    "        axs[i, 0].text(0.5, 0.98, date_info, transform=axs[i, 0].transAxes, \n",
    "                     ha='center', va='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        # Plot volatility\n",
    "        axs[i, 1].bar(tickers, [stats['volatility'][ticker] for ticker in tickers])\n",
    "        axs[i, 1].set_title(f'Regime {regime}: Volatility')\n",
    "        axs[i, 1].set_xlabel('Ticker')\n",
    "        axs[i, 1].set_ylabel('Volatility')\n",
    "        axs[i, 1].grid(True, alpha=0.3)\n",
    "        axs[i, 1].set_xticklabels(tickers, rotation=45)\n",
    "        \n",
    "        # Plot correlation matrix\n",
    "        corr_matrix = stats['correlation'].loc[tickers, tickers]\n",
    "        im = axs[i, 2].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axs[i, 2].set_title(f'Regime {regime}: Correlation Matrix')\n",
    "        axs[i, 2].set_xticks(np.arange(len(tickers)))\n",
    "        axs[i, 2].set_yticks(np.arange(len(tickers)))\n",
    "        axs[i, 2].set_xticklabels(tickers, rotation=45)\n",
    "        axs[i, 2].set_yticklabels(tickers)\n",
    "        plt.colorbar(im, ax=axs[i, 2])\n",
    "        \n",
    "        # Annotate the correlation matrix with the values\n",
    "        for ii in range(len(tickers)):\n",
    "            for jj in range(len(tickers)):\n",
    "                axs[i, 2].text(jj, ii, f\"{corr_matrix.iloc[ii, jj]:.2f}\", \n",
    "                             ha=\"center\", va=\"center\", color=\"black\" if abs(corr_matrix.iloc[ii, jj]) < 0.7 else \"white\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Compare with HMM-based regime detection\n",
    "def detect_regimes_with_hmm(X, num_regimes=4):\n",
    "    \"\"\"Detect market regimes using Hidden Markov Model.\"\"\"\n",
    "    # Initialize the HMM\n",
    "    hmm = GaussianHMM(n_components=num_regimes, covariance_type=\"full\", random_state=42, n_iter=100)\n",
    "    \n",
    "    # Reduce dimensionality first (HMM can struggle with high-dimensional data)\n",
    "    if X.shape[1] > 20:\n",
    "        pca = PCA(n_components=min(20, X.shape[1]))\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "    else:\n",
    "        X_reduced = X\n",
    "        \n",
    "    # Fit the HMM\n",
    "    hmm.fit(X_reduced)\n",
    "    \n",
    "    # Predict hidden states\n",
    "    hmm_regimes = hmm.predict(X_reduced)\n",
    "    \n",
    "    # Apply temporal smoothing to regime labels\n",
    "    window_size = 5\n",
    "    smoothed_regimes = np.copy(hmm_regimes)\n",
    "    \n",
    "    for i in range(len(hmm_regimes)):\n",
    "        start = max(0, i - window_size // 2)\n",
    "        end = min(len(hmm_regimes), i + window_size // 2 + 1)\n",
    "        window = hmm_regimes[start:end]\n",
    "        # Get most common regime in window\n",
    "        smoothed_regimes[i] = np.bincount(window).argmax()\n",
    "    \n",
    "    return smoothed_regimes\n",
    "\n",
    "# Compare different regime detection methods\n",
    "def compare_regime_methods(X, dates, returns_dict, ticker_to_plot=None):\n",
    "    \"\"\"Compare KAN vs HMM regime detection methods.\"\"\"\n",
    "    # Detect regimes using KAN (assuming model and params are already available)\n",
    "    print(\"Initializing the KAN model for regime detection...\")\n",
    "    input_dim = X.shape[1]\n",
    "    latent_dim = 5\n",
    "    model = RegimeDetectionKAN(input_dim, latent_dim, hidden_dims=[64, 32])\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training the KAN model...\")\n",
    "    trained_params, losses = train_regime_model(\n",
    "        model, X, num_epochs=100, lambda_smooth=1.0, lambda_reg=0.001)\n",
    "    \n",
    "    # Detect regimes using KAN\n",
    "    print(\"Detecting regimes with KAN...\")\n",
    "    kan_regimes, latent = detect_regimes(model, trained_params, X, num_regimes=4)\n",
    "    \n",
    "    # Detect regimes using HMM\n",
    "    print(\"Detecting regimes with HMM...\")\n",
    "    hmm_regimes = detect_regimes_with_hmm(X, num_regimes=4)\n",
    "    \n",
    "    # Create pandas Series for both methods\n",
    "    kan_series = pd.Series(kan_regimes, index=dates)\n",
    "    hmm_series = pd.Series(hmm_regimes, index=dates)\n",
    "    \n",
    "    # Choose a ticker to plot\n",
    "    if ticker_to_plot is None or ticker_to_plot not in returns_dict:\n",
    "        ticker_to_plot = list(returns_dict.keys())[0]\n",
    "    \n",
    "    # Get returns for the ticker\n",
    "    returns = returns_dict[ticker_to_plot]\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(16, 12), sharex=True, gridspec_kw={'height_ratios': [1, 1, 3]})\n",
    "    \n",
    "    # Plot KAN regimes\n",
    "    axs[0].plot(kan_series.index, kan_series.values, 'b-', linewidth=2)\n",
    "    axs[0].set_title('KAN Detected Regimes', fontsize=15)\n",
    "    axs[0].set_ylabel('Regime', fontsize=12)\n",
    "    axs[0].grid(True, alpha=0.3)\n",
    "    axs[0].set_yticks(np.unique(kan_regimes))\n",
    "    \n",
    "    # Plot HMM regimes\n",
    "    axs[1].plot(hmm_series.index, hmm_series.values, 'g-', linewidth=2)\n",
    "    axs[1].set_title('HMM Detected Regimes', fontsize=15)\n",
    "    axs[1].set_ylabel('Regime', fontsize=12)\n",
    "    axs[1].grid(True, alpha=0.3)\n",
    "    axs[1].set_yticks(np.unique(hmm_regimes))\n",
    "    \n",
    "    # Plot cumulative returns\n",
    "    axs[2].plot(cum_returns.index, cum_returns.values, 'k-', linewidth=1.5)\n",
    "    axs[2].set_title(f'Cumulative Returns for {ticker_to_plot}', fontsize=15)\n",
    "    axs[2].set_xlabel('Date', fontsize=12)\n",
    "    axs[2].set_ylabel('Cumulative Return', fontsize=12)\n",
    "    axs[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colored backgrounds for KAN regimes\n",
    "    unique_kan_regimes = np.unique(kan_regimes)\n",
    "    kan_colors = plt.cm.viridis(np.linspace(0, 1, len(unique_kan_regimes)))\n",
    "    \n",
    "    # Create a proper date-indexed Series\n",
    "    kan_filtered = kan_series.loc[kan_series.index.intersection(cum_returns.index)]\n",
    "    \n",
    "    # Process KAN regime periods\n",
    "    regime_changes = kan_filtered.diff().abs() > 0\n",
    "    change_points = kan_filtered.index[regime_changes]\n",
    "    \n",
    "    # Add start and end points\n",
    "    all_points = [kan_filtered.index[0]] + list(change_points) + [kan_filtered.index[-1]]\n",
    "    \n",
    "    # Color backgrounds for each period\n",
    "    for i in range(len(all_points) - 1):\n",
    "        start_date = all_points[i]\n",
    "        end_date = all_points[i+1]\n",
    "        regime = kan_filtered.loc[start_date]\n",
    "        axs[2].axvspan(start_date, end_date, alpha=0.2, color=kan_colors[regime])\n",
    "    \n",
    "    # Add a legend for the KAN regimes\n",
    "    kan_legend = [plt.Line2D([0], [0], color=kan_colors[i], lw=8, alpha=0.5, \n",
    "                          label=f'KAN Regime {i}') for i in unique_kan_regimes]\n",
    "    \n",
    "    # Add vertical lines for major market events\n",
    "    major_events = {\n",
    "        # Add relevant dates here, for example:\n",
    "        # '2020-03-23': 'COVID-19 Market Bottom',\n",
    "        # '2022-01-03': '2022 Market Peak',\n",
    "        # Customize based on your data period\n",
    "    }\n",
    "    \n",
    "    for date_str, event in major_events.items():\n",
    "        try:\n",
    "            event_date = pd.to_datetime(date_str)\n",
    "            if event_date in cum_returns.index:\n",
    "                for ax in axs:\n",
    "                    ax.axvline(event_date, color='r', linestyle='--', alpha=0.7)\n",
    "                    ax.text(event_date, ax.get_ylim()[1]*0.95, event, rotation=90, \n",
    "                           va='top', ha='right')\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Add the legend\n",
    "    axs[2].legend(handles=kan_legend, loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, model, trained_params, kan_regimes, hmm_regimes\n",
    "\n",
    "# Implement a simple trading strategy based on detected regimes\n",
    "def regime_based_trading_strategy(detected_regimes, dates, returns_dict):\n",
    "    \"\"\"Implement a simple trading strategy based on detected regimes.\"\"\"\n",
    "    # Create a pandas Series for the regimes\n",
    "    regime_series = pd.Series(detected_regimes, index=dates)\n",
    "    \n",
    "    # Initialize a dictionary to store strategy results\n",
    "    strategy_results = {}\n",
    "    \n",
    "    # For each ticker, implement a strategy\n",
    "    for ticker in returns_dict:\n",
    "        # Get returns for this ticker\n",
    "        ticker_returns = returns_dict[ticker].reindex(regime_series.index)\n",
    "        \n",
    "        # Calculate strategy for each regime\n",
    "        regime_stats = {}\n",
    "        for regime in np.unique(detected_regimes):\n",
    "            regime_returns = ticker_returns[regime_series == regime]\n",
    "            regime_stats[regime] = {\n",
    "                'mean_return': regime_returns.mean(),\n",
    "                'volatility': regime_returns.std(),\n",
    "                'sharpe': regime_returns.mean() / regime_returns.std() if regime_returns.std() > 0 else 0\n",
    "            }\n",
    "        \n",
    "        # Determine the strategy: long in regimes with positive expected returns, short in regimes with negative returns\n",
    "        positions = np.zeros(len(regime_series))\n",
    "        \n",
    "        for i, (date, regime) in enumerate(regime_series.items()):\n",
    "            if regime_stats[regime]['mean_return'] > 0:\n",
    "                positions[i] = 1  # Long\n",
    "            elif regime_stats[regime]['mean_return'] < 0:\n",
    "                positions[i] = -1  # Short\n",
    "            # else: 0 = cash\n",
    "        \n",
    "        # Calculate strategy returns\n",
    "        strategy_returns = positions * ticker_returns.values\n",
    "        \n",
    "        # Calculate cumulative returns\n",
    "        cum_strategy_returns = (1 + pd.Series(strategy_returns, index=regime_series.index)).cumprod()\n",
    "        cum_buy_hold_returns = (1 + ticker_returns).cumprod()\n",
    "        \n",
    "        # Store results\n",
    "        strategy_results[ticker] = {\n",
    "            'positions': positions,\n",
    "            'strategy_returns': strategy_returns,\n",
    "            'cum_strategy_returns': cum_strategy_returns,\n",
    "            'cum_buy_hold_returns': cum_buy_hold_returns,\n",
    "            'regime_stats': regime_stats\n",
    "        }\n",
    "    \n",
    "    return strategy_results\n",
    "\n",
    "# Visualize trading strategy results\n",
    "def visualize_trading_strategy(strategy_results, ticker, detected_regimes, dates):\n",
    "    \"\"\"Visualize the performance of the regime-based trading strategy.\"\"\"\n",
    "    # Get results for the specified ticker\n",
    "    results = strategy_results[ticker]\n",
    "    \n",
    "    # Create a pandas Series for the regimes\n",
    "    regime_series = pd.Series(detected_regimes, index=dates)\n",
    "    \n",
    "    # Create figure with three subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(16, 12), sharex=True, gridspec_kw={'height_ratios': [1, 1, 3]})\n",
    "    \n",
    "    # Plot regimes\n",
    "    axs[0].plot(regime_series.index, regime_series.values, 'k-', linewidth=2)\n",
    "    axs[0].set_title('Detected Market Regimes', fontsize=15)\n",
    "    axs[0].set_ylabel('Regime', fontsize=12)\n",
    "    axs[0].grid(True, alpha=0.3)\n",
    "    axs[0].set_yticks(np.unique(detected_regimes))\n",
    "    \n",
    "    # Plot positions\n",
    "    positions_series = pd.Series(results['positions'], index=dates)\n",
    "    axs[1].plot(positions_series.index, positions_series.values, 'b-', linewidth=2)\n",
    "    axs[1].set_title('Trading Positions', fontsize=15)\n",
    "    axs[1].set_ylabel('Position', fontsize=12)\n",
    "    axs[1].grid(True, alpha=0.3)\n",
    "    axs[1].set_yticks([-1, 0, 1])\n",
    "    axs[1].set_yticklabels(['Short', 'Cash', 'Long'])\n",
    "    \n",
    "    # Plot cumulative returns\n",
    "    axs[2].plot(results['cum_strategy_returns'].index, results['cum_strategy_returns'].values, 'g-', linewidth=2, label='Regime Strategy')\n",
    "    axs[2].plot(results['cum_buy_hold_returns'].index, results['cum_buy_hold_returns'].values, 'r--', linewidth=2, label='Buy & Hold')\n",
    "    axs[2].set_title(f'Cumulative Returns for {ticker}', fontsize=15)\n",
    "    axs[2].set_xlabel('Date', fontsize=12)\n",
    "    axs[2].set_ylabel('Cumulative Return', fontsize=12)\n",
    "    axs[2].grid(True, alpha=0.3)\n",
    "    axs[2].legend(loc='lower right')\n",
    "    \n",
    "    # Add colored backgrounds for different regimes\n",
    "    unique_regimes = np.unique(detected_regimes)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_regimes)))\n",
    "    \n",
    "    # Find regime change points\n",
    "    regime_filtered = regime_series.loc[regime_series.index.intersection(results['cum_strategy_returns'].index)]\n",
    "    regime_changes = regime_filtered.diff().abs() > 0\n",
    "    change_points = regime_filtered.index[regime_changes]\n",
    "    \n",
    "    # Add start and end points\n",
    "    all_points = [regime_filtered.index[0]] + list(change_points) + [regime_filtered.index[-1]]\n",
    "    \n",
    "    # Color backgrounds for each period\n",
    "    for i in range(len(all_points) - 1):\n",
    "        start_date = all_points[i]\n",
    "        end_date = all_points[i+1]\n",
    "        regime = regime_filtered.loc[start_date]\n",
    "        for ax in axs:\n",
    "            ax.axvspan(start_date, end_date, alpha=0.2, color=colors[regime])\n",
    "    \n",
    "    # Add a legend for the regimes\n",
    "    regime_legend = [plt.Line2D([0], [0], color=colors[i], lw=8, alpha=0.5, \n",
    "                          label=f'Regime {i}') for i in unique_regimes]\n",
    "    axs[2].legend(handles=[\n",
    "        plt.Line2D([0], [0], color='g', lw=2, label='Regime Strategy'),\n",
    "        plt.Line2D([0], [0], color='r', linestyle='--', lw=2, label='Buy & Hold')\n",
    "    ] + regime_legend, loc='lower right')\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    strategy_total_return = results['cum_strategy_returns'].iloc[-1] - 1\n",
    "    buy_hold_total_return = results['cum_buy_hold_returns'].iloc[-1] - 1\n",
    "    \n",
    "    strategy_returns_series = pd.Series(results['strategy_returns'], index=dates)\n",
    "    buy_hold_returns_series = results['cum_buy_hold_returns'].pct_change().fillna(0)\n",
    "    \n",
    "    strategy_sharpe = strategy_returns_series.mean() / strategy_returns_series.std() * np.sqrt(252)\n",
    "    buy_hold_sharpe = buy_hold_returns_series.mean() / buy_hold_returns_series.std() * np.sqrt(252)\n",
    "    \n",
    "    # Function to calculate max drawdown\n",
    "    def max_drawdown(returns):\n",
    "        cum_returns = (1 + returns).cumprod()\n",
    "        running_max = cum_returns.cummax()\n",
    "        drawdown = (cum_returns / running_max) - 1\n",
    "        return drawdown.min()\n",
    "    \n",
    "    strategy_max_dd = max_drawdown(strategy_returns_series)\n",
    "    buy_hold_max_dd = max_drawdown(buy_hold_returns_series)\n",
    "    \n",
    "    # Add performance metrics as text\n",
    "    metrics_text = (\n",
    "        f\"Strategy Return: {strategy_total_return:.2%}\\n\"\n",
    "        f\"Buy & Hold Return: {buy_hold_total_return:.2%}\\n\"\n",
    "        f\"Strategy Sharpe: {strategy_sharpe:.2f}\\n\"\n",
    "        f\"Buy & Hold Sharpe: {buy_hold_sharpe:.2f}\\n\"\n",
    "        f\"Strategy Max DD: {strategy_max_dd:.2%}\\n\"\n",
    "        f\"Buy & Hold Max DD: {buy_hold_max_dd:.2%}\\n\"\n",
    "    )\n",
    "    \n",
    "    # Add text box with metrics\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "    axs[2].text(0.02, 0.98, metrics_text, transform=axs[2].transAxes, fontsize=10,\n",
    "               verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Main function to run the market regime detection\n",
    "def main():\n",
    "    # Parameters\n",
    "    tickers = ['SPY', 'QQQ', 'TLT', 'GLD', '^FTSE']  # Example tickers\n",
    "    end_date = datetime.now() #- timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=5*365)  # 5 years of data\n",
    "    window_size = 20\n",
    "    num_regimes = 4\n",
    "    \n",
    "    # Download and prepare data\n",
    "    print(f\"Downloading data for {tickers} from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "    X, dates, returns_dict = download_market_data(tickers, start_date, end_date, feature_window=window_size)\n",
    "    \n",
    "    # Detect regimes using KAN and HMM\n",
    "    print(\"Comparing regime detection methods...\")\n",
    "    comparison_fig, model, trained_params, kan_regimes, hmm_regimes = compare_regime_methods(X, dates, returns_dict, 'SPY')\n",
    "    \n",
    "    # Analyze regime characteristics\n",
    "    print(\"Analyzing KAN regime characteristics...\")\n",
    "    regime_stats = analyze_regime_characteristics(kan_regimes, dates, returns_dict)\n",
    "    \n",
    "    # Visualize regime characteristics\n",
    "    print(\"Visualizing regime characteristics...\")\n",
    "    characteristics_fig = visualize_regime_characteristics(regime_stats, tickers)\n",
    "    \n",
    "    # Implement trading strategy\n",
    "    print(\"Implementing regime-based trading strategy...\")\n",
    "    strategy_results = regime_based_trading_strategy(kan_regimes, dates, returns_dict)\n",
    "    \n",
    "    # Visualize strategy results for SPY\n",
    "    print(\"Visualizing trading strategy results...\")\n",
    "    strategy_fig = visualize_trading_strategy(strategy_results, 'SPY', kan_regimes, dates)\n",
    "\n",
    "    strategy_fig = visualize_trading_strategy(strategy_results, 'QQQ', kan_regimes, dates)\n",
    "\n",
    "    strategy_fig = visualize_trading_strategy(strategy_results, 'TLT', kan_regimes, dates)\n",
    "\n",
    "    strategy_fig = visualize_trading_strategy(strategy_results, 'GLD', kan_regimes, dates)\n",
    "\n",
    "    strategy_fig = visualize_trading_strategy(strategy_results, '^FTSE', kan_regimes, dates)\n",
    "    \n",
    "    # Return results\n",
    "    return model, trained_params, kan_regimes, hmm_regimes, regime_stats, strategy_results, comparison_fig, characteristics_fig, strategy_fig\n",
    "\n",
    "# Entry point for the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
