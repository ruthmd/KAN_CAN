{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a54f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optax yfinance lxml plotly hmmlearn \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6370e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, jit\n",
    "import numpy as np\n",
    "import optax\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Callable, Any\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "# Define the KAN Layer for yield curve modeling\n",
    "class KANLayer:\n",
    "    def __init__(self, input_dim: int, output_dim: int, num_basis: int = 20, \n",
    "                 domain=(-3.0, 3.0), key=None):\n",
    "        \"\"\"Initialize a KAN layer with learnable activation functions.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input dimension\n",
    "            output_dim: Output dimension\n",
    "            num_basis: Number of basis functions for learned activation\n",
    "            domain: Domain over which the activation functions are defined\n",
    "            key: JAX random key\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        \n",
    "        key1, key2 = jax.random.split(key)\n",
    "        \n",
    "        # Initialize weights for linear transformation\n",
    "        self.weights = jax.random.normal(key1, (input_dim, output_dim)) * 0.1\n",
    "        \n",
    "        # Initialize biases\n",
    "        self.biases = jax.random.normal(key2, (output_dim,)) * 0.01\n",
    "        \n",
    "        # Grid points for activation function representation\n",
    "        self.grid_points = jnp.linspace(domain[0], domain[1], num_basis)\n",
    "        \n",
    "        # Initialize activation function values at grid points\n",
    "        # For yield curves, we'll start with something that can model exponential decay\n",
    "        # and other common yield curve shapes\n",
    "        key3 = jax.random.split(key2)[0]\n",
    "        \n",
    "        # Initialize different activation patterns for different outputs\n",
    "        activations_list = []\n",
    "        for i in range(output_dim):\n",
    "            if i % 3 == 0:  # Exponential-like decay (common in yield curves)\n",
    "                act = jnp.exp(-0.5 * self.grid_points) * (self.grid_points > 0)\n",
    "            elif i % 3 == 1:  # Hump-shaped (for modeling yield curve humps)\n",
    "                act = jnp.exp(-0.5 * (self.grid_points - 1.0)**2)\n",
    "            else:  # More flexible activation\n",
    "                act = 0.5 * (1 + jnp.tanh(self.grid_points))\n",
    "            \n",
    "            # Add some random noise to break symmetry\n",
    "            act = act + jax.random.normal(key3, (num_basis,)) * 0.01\n",
    "            activations_list.append(act)\n",
    "        \n",
    "        # Stack into a matrix: (output_dim, num_basis)\n",
    "        self.activations = jnp.stack(activations_list)\n",
    "        \n",
    "        # Store domain for clipping\n",
    "        self.domain = domain\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass through the KAN layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # Linear transformation\n",
    "        z = jnp.dot(x, self.weights) + self.biases  # Shape: (batch_size, output_dim)\n",
    "        \n",
    "        # Apply learned activation functions\n",
    "        # First, clip inputs to domain\n",
    "        z_clipped = jnp.clip(z, self.domain[0], self.domain[1])\n",
    "        \n",
    "        # For each output dimension, interpolate activation function\n",
    "        def apply_activation(z_i, i):\n",
    "            \"\"\"Apply the i-th activation function to z_i.\"\"\"\n",
    "            # Linear interpolation between grid points\n",
    "            idx = jnp.searchsorted(self.grid_points, z_i) - 1\n",
    "            idx = jnp.clip(idx, 0, len(self.grid_points) - 2)\n",
    "            \n",
    "            # Get surrounding points\n",
    "            x0 = self.grid_points[idx]\n",
    "            x1 = self.grid_points[idx + 1]\n",
    "            y0 = self.activations[i, idx]\n",
    "            y1 = self.activations[i, idx + 1]\n",
    "            \n",
    "            # Linear interpolation: y = y0 + (y1 - y0) * (x - x0) / (x1 - x0)\n",
    "            t = (z_i - x0) / (x1 - x0)\n",
    "            return y0 + t * (y1 - y0)\n",
    "        \n",
    "        # Apply activation function for each element in the batch and each output dimension\n",
    "        output = jnp.zeros_like(z)\n",
    "        for i in range(z.shape[1]):  # For each output dimension\n",
    "            output = output.at[:, i].set(vmap(lambda z_i: apply_activation(z_i, i))(z[:, i]))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Full KAN model for yield curve modeling\n",
    "class YieldCurveKAN:\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: List[int] = [64, 32], \n",
    "                 num_basis: int = 40, domain=(-3.0, 3.0), key=None):\n",
    "        \"\"\"Initialize a KAN model for yield curve modeling.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input dimension (typically factors affecting yield curve)\n",
    "            output_dim: Output dimension (typically different tenors on the yield curve)\n",
    "            hidden_dims: List of hidden dimensions\n",
    "            num_basis: Number of basis functions for learned activations\n",
    "            domain: Domain for activation functions\n",
    "            key: JAX random key\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        \n",
    "        keys = jax.random.split(key, len(hidden_dims) + 1)\n",
    "        \n",
    "        # Initialize layers\n",
    "        self.layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layer = KANLayer(prev_dim, hidden_dim, num_basis, domain, keys[i])\n",
    "            self.layers.append(layer)\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Final layer specifically designed for yield curve output\n",
    "        self.output_layer = KANLayer(prev_dim, output_dim, num_basis, domain, keys[-1])\n",
    "    \n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Forward pass through the KAN model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, output_dim) representing yield curves\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Apply output layer\n",
    "        y = self.output_layer(x)\n",
    "        \n",
    "        # Ensure yield curve outputs are positive (yields are typically positive)\n",
    "        # We use softplus for positivity with a small scaling factor to prevent zeros\n",
    "        return jax.nn.softplus(y)\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"Get model parameters as a flat dictionary.\"\"\"\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            params[f'layer_{i}_weights'] = layer.weights\n",
    "            params[f'layer_{i}_biases'] = layer.biases\n",
    "            params[f'layer_{i}_activations'] = layer.activations\n",
    "        \n",
    "        params['output_layer_weights'] = self.output_layer.weights\n",
    "        params['output_layer_biases'] = self.output_layer.biases\n",
    "        params['output_layer_activations'] = self.output_layer.activations\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def update_params(self, params):\n",
    "        \"\"\"Update model parameters from a flat dictionary.\"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer.weights = params[f'layer_{i}_weights']\n",
    "            layer.biases = params[f'layer_{i}_biases']\n",
    "            layer.activations = params[f'layer_{i}_activations']\n",
    "        \n",
    "        self.output_layer.weights = params['output_layer_weights']\n",
    "        self.output_layer.biases = params['output_layer_biases']\n",
    "        self.output_layer.activations = params['output_layer_activations']\n",
    "\n",
    "# Nelson-Siegel-Svensson model for yield curve generation\n",
    "def nelson_siegel_svensson(t, beta0, beta1, beta2, beta3, tau1, tau2):\n",
    "    \"\"\"\n",
    "    Nelson-Siegel-Svensson model for yield curve generation.\n",
    "    \n",
    "    Args:\n",
    "        t: Array of tenors (time to maturity)\n",
    "        beta0, beta1, beta2, beta3: Shape parameters\n",
    "        tau1, tau2: Time constants\n",
    "    \n",
    "    Returns:\n",
    "        Yield curve rates for each tenor\n",
    "    \"\"\"\n",
    "    # Handle division by zero for small tenors\n",
    "    t_safe = jnp.maximum(t, 1e-10)\n",
    "    \n",
    "    # Calculate terms\n",
    "    term1 = 1.0\n",
    "    term2 = (1.0 - jnp.exp(-t_safe / tau1)) / (t_safe / tau1)\n",
    "    term3 = term2 - jnp.exp(-t_safe / tau1)\n",
    "    term4 = (1.0 - jnp.exp(-t_safe / tau2)) / (t_safe / tau2) - jnp.exp(-t_safe / tau2)\n",
    "    \n",
    "    # Combine terms to get yield curve\n",
    "    y = beta0 * term1 + beta1 * term2 + beta2 * term3 + beta3 * term4\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Generate synthetic yield curve data for training\n",
    "def generate_yield_curve_data(num_samples=100000, num_tenors=10):\n",
    "    \"\"\"\n",
    "    Generate synthetic yield curve data using Nelson-Siegel-Svensson model.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of yield curves to generate\n",
    "        num_tenors: Number of points on each yield curve\n",
    "    \n",
    "    Returns:\n",
    "        X: Economic factors driving the yield curve\n",
    "        Y: Yield curves\n",
    "    \"\"\"\n",
    "    key = jax.random.PRNGKey(123)\n",
    "    keys = jax.random.split(key, 8)\n",
    "    \n",
    "    # Define tenors (in years)\n",
    "    tenors = jnp.array([0.25, 0.5, 1.0, 2.0, 3.0, 5.0, 7.0, 10.0, 20.0, 30.0])[:num_tenors]\n",
    "    \n",
    "    # Generate random parameters for Nelson-Siegel-Svensson model\n",
    "    beta0 = jax.random.uniform(keys[0], (num_samples,), minval=0.01, maxval=0.05)  # Long-term rate\n",
    "    beta1 = jax.random.uniform(keys[1], (num_samples,), minval=-0.03, maxval=0.03)  # Short-term component\n",
    "    beta2 = jax.random.uniform(keys[2], (num_samples,), minval=-0.03, maxval=0.03)  # Medium-term component\n",
    "    beta3 = jax.random.uniform(keys[3], (num_samples,), minval=-0.03, maxval=0.03)  # Second hump component\n",
    "    tau1 = jax.random.uniform(keys[4], (num_samples,), minval=0.5, maxval=3.0)  # First time constant\n",
    "    tau2 = jax.random.uniform(keys[5], (num_samples,), minval=3.0, maxval=10.0)  # Second time constant\n",
    "    \n",
    "    # Economic factors that drive yield curves (for input to our model)\n",
    "    # In practice, these would be macroeconomic variables like inflation, GDP growth, etc.\n",
    "    # Here we'll generate synthetic factors that correlate with our NSS parameters\n",
    "    num_factors = 5  # Number of economic factors\n",
    "    \n",
    "    # Create correlations between economic factors and NSS parameters\n",
    "    factor_noise = jax.random.normal(keys[6], (num_samples, num_factors)) * 0.2\n",
    "    \n",
    "    # Factor 1: Correlates with level (beta0)\n",
    "    # Factor 2: Correlates with slope (beta1)\n",
    "    # Factor 3: Correlates with curvature (beta2)\n",
    "    # Factors 4-5: Additional factors with some correlation to all parameters\n",
    "    \n",
    "    factors = jnp.zeros((num_samples, num_factors))\n",
    "    factors = factors.at[:, 0].set(beta0 * 20.0 + factor_noise[:, 0])  # Level factor\n",
    "    factors = factors.at[:, 1].set(beta1 * 20.0 + factor_noise[:, 1])  # Slope factor\n",
    "    factors = factors.at[:, 2].set(beta2 * 20.0 + factor_noise[:, 2])  # Curvature factor\n",
    "    factors = factors.at[:, 3].set(beta3 * 10.0 + beta0 * 5.0 + factor_noise[:, 3])  # Mixed factor 1\n",
    "    factors = factors.at[:, 4].set(tau1 * 0.5 - tau2 * 0.2 + factor_noise[:, 4])  # Mixed factor 2\n",
    "    \n",
    "    # Generate yield curves using Nelson-Siegel-Svensson model\n",
    "    yield_curves = jnp.zeros((num_samples, num_tenors))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        yield_curves = yield_curves.at[i].set(\n",
    "            nelson_siegel_svensson(tenors, beta0[i], beta1[i], beta2[i], beta3[i], tau1[i], tau2[i])\n",
    "        )\n",
    "    \n",
    "    return factors, yield_curves, tenors\n",
    "\n",
    "# Training functions\n",
    "@jit\n",
    "def loss_fn(params, X, Y):\n",
    "    \"\"\"Mean squared error loss function for yield curve prediction.\"\"\"\n",
    "    model = YieldCurveKAN(X.shape[1], Y.shape[1])\n",
    "    model.update_params(params)\n",
    "    pred = model(X)\n",
    "    return jnp.mean((pred - Y) ** 2)\n",
    "\n",
    "@jit\n",
    "def train_step(params, X, Y, opt_state):\n",
    "    \"\"\"Single optimization step.\"\"\"\n",
    "    loss_value, grads = jax.value_and_grad(loss_fn)(params, X, Y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_value\n",
    "\n",
    "def train_model(X, Y, params, num_epochs=100, batch_size=64):\n",
    "    \"\"\"Train the model for a specified number of epochs.\"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    # Initialize optimizer state\n",
    "    opt_state = optimizer.init(params)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle data\n",
    "        perm = jax.random.permutation(jax.random.PRNGKey(epoch), num_samples)\n",
    "        X_shuffled = X[perm]\n",
    "        Y_shuffled = Y[perm]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            \n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            Y_batch = Y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            params, opt_state, batch_loss = train_step(params, X_batch, Y_batch, opt_state)\n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss:.6f}\")\n",
    "    \n",
    "    return params, losses\n",
    "\n",
    "# Post-training analysis and visualization\n",
    "def analyze_yield_curve_model(model, params, X_test, Y_test, tenors):\n",
    "    \"\"\"Analyze and visualize the trained yield curve model.\"\"\"\n",
    "    # Update model with trained parameters\n",
    "    model.update_params(params)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model(X_test)\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    mae = jnp.mean(jnp.abs(predictions - Y_test))\n",
    "    print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "    \n",
    "    # Plot actual vs. predicted yield curves for a few examples\n",
    "    num_examples = min(5, X_test.shape[0])\n",
    "    \n",
    "    fig, axs = plt.subplots(num_examples, 1, figsize=(10, 3 * num_examples))\n",
    "    if num_examples == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        axs[i].plot(tenors, Y_test[i], 'b-o', label='Actual')\n",
    "        axs[i].plot(tenors, predictions[i], 'r--x', label='Predicted')\n",
    "        axs[i].set_title(f'Yield Curve Example {i+1}')\n",
    "        axs[i].set_xlabel('Tenor (years)')\n",
    "        axs[i].set_ylabel('Yield (%)')\n",
    "        axs[i].grid(True)\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Analyze the learned activation functions\n",
    "    # These can provide insights into how the model is capturing yield curve shapes\n",
    "    fig2, axs2 = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Plot a few activation functions from the output layer\n",
    "    grid_points = model.output_layer.grid_points\n",
    "    output_activations = model.output_layer.activations\n",
    "    \n",
    "    for i in range(min(4, output_activations.shape[0])):\n",
    "        row, col = i // 2, i % 2\n",
    "        axs2[row, col].plot(grid_points, output_activations[i])\n",
    "        axs2[row, col].set_title(f'Learned Activation for Tenor {tenors[i]:.1f}y')\n",
    "        axs2[row, col].set_xlabel('Input')\n",
    "        axs2[row, col].set_ylabel('Activation')\n",
    "        axs2[row, col].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Calculate and visualize principal components of yield curve dynamics\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Convert to numpy for PCA\n",
    "    Y_np = np.array(Y_test)\n",
    "    \n",
    "    # Fit PCA to the yield curves\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(Y_np)\n",
    "    \n",
    "    # Plot the principal components\n",
    "    fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    component_names = ['Level', 'Slope', 'Curvature']\n",
    "    for i in range(3):\n",
    "        ax3.plot(tenors, pca.components_[i], label=component_names[i])\n",
    "    \n",
    "    ax3.set_title('Principal Components of Yield Curve Dynamics')\n",
    "    ax3.set_xlabel('Tenor (years)')\n",
    "    ax3.set_ylabel('Loading')\n",
    "    ax3.grid(True)\n",
    "    ax3.legend()\n",
    "    \n",
    "    return fig, fig2, fig3, mae\n",
    "\n",
    "# Example: Calculate yield curve derivatives for risk management\n",
    "def calculate_yield_curve_derivatives(model, params, X):\n",
    "    \"\"\"Calculate derivatives of yield curves with respect to input factors.\"\"\"\n",
    "    # Update model with parameters\n",
    "    model.update_params(params)\n",
    "    \n",
    "    # Create a function that returns the yield curve given input factors\n",
    "    def yield_curve_fn(x):\n",
    "        return model(x.reshape(1, -1))[0]\n",
    "    \n",
    "    # Calculate Jacobian (sensitivities to all factors)\n",
    "    jacobian_fn = jax.jacfwd(yield_curve_fn)\n",
    "    \n",
    "    # Calculate sensitivities for each example\n",
    "    sensitivities = jax.vmap(jacobian_fn)(X)\n",
    "    \n",
    "    # sensitivities shape: (num_samples, num_tenors, num_factors)\n",
    "    return sensitivities\n",
    "\n",
    "# Yield curve interpolation and extrapolation\n",
    "def interpolate_yield_curve(model, params, X, target_tenors, original_tenors):\n",
    "    \"\"\"Interpolate yield curves to arbitrary tenors.\"\"\"\n",
    "    # First, get the predicted yield curves at the original tenors\n",
    "    model.update_params(params)\n",
    "    predicted_curves = model(X)\n",
    "    \n",
    "    # Helper function to interpolate a single yield curve\n",
    "    def interpolate_single_curve(curve):\n",
    "        # Use cubic spline interpolation\n",
    "        from scipy.interpolate import CubicSpline\n",
    "        cs = CubicSpline(original_tenors, curve)\n",
    "        return cs(target_tenors)\n",
    "    \n",
    "    # Apply to each curve\n",
    "    interpolated_curves = []\n",
    "    for i in range(predicted_curves.shape[0]):\n",
    "        curve = np.array(predicted_curves[i])\n",
    "        interpolated = interpolate_single_curve(curve)\n",
    "        interpolated_curves.append(interpolated)\n",
    "    \n",
    "    return np.array(interpolated_curves)\n",
    "\n",
    "# Main function to run everything\n",
    "def main():\n",
    "    # Generate synthetic data\n",
    "    print(\"Generating synthetic yield curve data...\")\n",
    "    X, Y, tenors = generate_yield_curve_data(num_samples=2000, num_tenors=10)\n",
    "    \n",
    "    # Split into train/test sets\n",
    "    train_size = int(0.8 * X.shape[0])\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing KAN model for yield curve modeling...\")\n",
    "    input_dim = X.shape[1]    # Number of economic factors\n",
    "    output_dim = Y.shape[1]   # Number of tenors\n",
    "    \n",
    "    model = YieldCurveKAN(input_dim, output_dim, hidden_dims=[32, 16], num_basis=40)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    global optimizer  # Make accessible to JIT-compiled functions\n",
    "    learning_rate = 0.001\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    trained_params, losses = train_model(X_train, Y_train, model.params, num_epochs=100)\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"Analyzing results...\")\n",
    "    fig1, fig2, fig3, mae = analyze_yield_curve_model(model, trained_params, X_test, Y_test, tenors)\n",
    "    \n",
    "    # Calculate sensitivities\n",
    "    print(\"Calculating yield curve sensitivities...\")\n",
    "    sensitivities = calculate_yield_curve_derivatives(model, trained_params, X_test[:5])\n",
    "    \n",
    "    # Return results\n",
    "    return model, trained_params, losses, fig1, fig2, fig3, sensitivities\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
